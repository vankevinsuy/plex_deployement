apiVersion: v1
kind: Namespace
metadata:
  name: plexserver
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: plexserver-pv-nfs-config   # < name of the persisant volume ("pv") in kubenetes
  namespace: plexserver            # < namespace where place the pv
spec:
  storageClassName: ""
  capacity:
    storage: 1Gi                   # < max. size we reserve for the pv
  accessModes:
    - ReadWriteMany                # < Multiple pods can write to storage 
  persistentVolumeReclaimPolicy: Retain # < The persistent volume can reclaimed 
  hostPath:
    path: /mnt/ssd/plex/config            # < Name of your NFS share with subfolder
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: plexserver-pv-nfs-data
  namespace: plexserver
spec:
  storageClassName: ""
  capacity:
    storage: 10Gi                   # < max. size we reserve for the pv. A bigger value than the configdata
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/ssd/plex/data
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: plexserver-pvc-config   # < name of the persistant volume claim ("pvc'")
  namespace: plexserver         # < namespace where place the pvc
spec:
  storageClassName: ""
  volumeName: plexserver-pv-nfs-config  # < the pv it will "claim" to storage. Created in the previous yaml.
  accessModes:
    - ReadWriteMany             # < Multiple pods can write to storage. Same value as pv
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi              # < How much data can the pvc claim from pv
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: plexserver-pvc-data
  namespace: plexserver
spec:
  storageClassName: ""
  volumeName: plexserver-pv-nfs-data
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: plexserver               # < label for tagging and reference
  name: plexserver                # < name of the deployment
  namespace: plexserver           # < namespace where to place the deployment and pods               # < namespace where place the deployment and pods
spec:
  replicas: 1                     # < number of pods to deploy
  revisionHistoryLimit: 0
  selector:
    matchLabels:
      app: plexserver
  strategy:
    rollingUpdate:
      maxSurge: 0                 # < The number of pods that can be created above the desired amount of pods during an update
      maxUnavailable: 1           # < The number of pods that can be unavailable during the update process
    type: RollingUpdate           # < New pods are added gradually, and old pods are terminated gradually
  template:
    metadata:
      labels:
        app: plexserver
    spec:
      volumes:
      - name: nfs-plex-config     # < linkname of the volume for the pvc
        persistentVolumeClaim:
          claimName: plexserver-pvc-config  # < pvc name we created in the previous yaml
      - name: nfs-data
        persistentVolumeClaim:
          claimName: plexserver-pvc-data
      containers:
      - env:                       # < environment variables. See https://hub.docker.com/r/linuxserver/plex
        # - name: PLEX_CLAIM
        #   value: claim-XwVPsHsaakdfaq66tha9
        - name: PGID
          value: "1000"    # < ASCII code for '100'
        - name: PUID
          value: "1000" # < ACII code for '1035'
        - name: VERSION
          value: docker
        - name: TZ
          value: US/Eastern  # < Timezone
        image: linuxserver/plex:latest  # < the name of the docker image we will use
        imagePullPolicy: Always    # < always use the latest image when creating container/pod
        name: plexserver           # < name of container
        ports:
        - containerPort: 32400     # < required network portnumber. See https://hub.docker.com/r/linuxserver/plex
          name: pms-web            # < reference name from the port in the service yaml
          protocol: TCP
        # - containerPort: 32469
        #   name: dlna-tcp
        #   protocol: TCP
        # - containerPort: 1900
        #   name: dlna-udp
        #   protocol: UDP
        # - containerPort: 3005
        #   name: plex-companion
        #   protocol: TCP  
        # - containerPort: 5353
        #   name: discovery-udp
        #   protocol: UDP  
        # - containerPort: 8324
        #   name: plex-roku
        #   protocol: TCP  
        # - containerPort: 32410
        #   name: gdm-32410
        #   protocol: UDP
        # - containerPort: 32412
        #   name: gdm-32412
        #   protocol: UDP
        # - containerPort: 32413
        #   name: gdm-32413
        #   protocol: UDP
        # - containerPort: 32414
        #   name: gdm-32414
        #   protocol: UDP
        resources: {}
        stdin: true
        tty: true
        volumeMounts:            # < the volume mount in the container. Look at the relation volumelabel->pvc->pv
        - mountPath: /config     # < mount location in the container
          name: nfs-plex-config  # < volumelabel configured earlier in the yaml file
        - mountPath: /data
          name: nfs-data 
      restartPolicy: Always
---
# kind: Service
# apiVersion: v1
# metadata:
#   name: plex-udp              # < name of the service
#   namespace: plexserver       # < namespace where to place service
#   annotations:
#     metallb.universe.tf/allow-shared-ip: plexserver # < annotation name to combine the Service IP, make sure it's same name as in the service UDP yaml
# spec:
#   selector:
#     app: plexserver           # < reference to the deployment (connects the service with the deployment)
#   ports:
#   - port: 1900                # < port to open on the outside on the server
#     targetPort: 1900          # < targetport. port on the pod to passthrough
#     name: dlna-udp            # < reference name for the port in the deployment yaml
#     protocol: UDP
#   - port: 5353
#     targetPort: 5353
#     name: discovery-udp
#     protocol: UDP
#   - port: 32410
#     targetPort: 32410
#     name: gdm-32410
#     protocol: UDP
#   - port: 32412
#     targetPort: 32412
#     name: gdm-32412
#     protocol: UDP
#   - port: 32413
#     targetPort: 32413
#     name: gdm-32413
#     protocol: UDP
#   - port: 32414
#     targetPort: 32414
#     name: gdm-32414
#     protocol: UDP
#   type: LoadBalancer
#   # loadBalancerIP: xxx.xxx.xxx.xxx  # < IP to access your plexserver. Should be one from the MetalLB range and the same as the UDP yaml
# ---
kind: Service
apiVersion: v1
metadata:
  name: plex-tcp              # < name of the service
  namespace: plexserver       # < namespace where to place service
  annotations:
    metallb.universe.tf/allow-shared-ip: plexserver  # < annotation name to combine the Service IP, make sure it's same name as in the service UDP yaml
spec:
  selector:
    app: plexserver           # < reference to the deployment (connects the service with the deployment)
  ports:                      
  - port: 32400               # < port to open on the outside on the server
    targetPort: 32400         # < targetport. port on the pod to passthrough
    name: pms-web             # < reference name for the port in the deployment yaml
    protocol: TCP
  # - port: 3005
  #   targetPort: 3005
  #   name: plex-companion
  # - port: 8324
  #   name: plex-roku
  #   targetPort: 8324  
  #   protocol: TCP  
  # - port: 32469
  #   targetPort: 32469
  #   name: dlna-tcp
  #   protocol: TCP
  type: LoadBalancer
  # loadBalancerIP: xxx.xxx.xxx.xxx  # < IP to access your plexserver. Should be one from the MetalLB range and the same as the TCP yaml
---  
# apiVersion: networking.k8s.io/v1
# kind: Ingress
# metadata:
#   name: plexserver            # < name of ingress entry
#   namespace: plexserver       # < namespace where place the ingress enty

#   annotations:
#     kubernetes.io/ingress.class: "nginx"  # < use the nginx ingress controller
#     nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"  # < communicate in https with the backend (service/pod)
#     cert-manager.io/cluster-issuer: "letsencrypt-prod"     # < use letsencrypt-prod application in kubernetes to generate ssl certificate
#     nginx.ingress.kubernetes.io/app-root: /web             # < the root directory of the plex webserver
# spec:
#   rules:
#   - host: snoozyhomelab.com
#     http:
#       paths:
#       - path: /
#         backend:
#           serviceName: plex-tcp
#           servicePort: pms-web   # < same label as the port in the service tcp file
#   tls: # < placing a host in the TLS config will indicate a cert should be created
#   - hosts:
#     - snoozyhomelab.com
#     secretName: snoozyhomelab.com-tls # < cert-manager will store the created certificate in this secret.
    